
async function _{{{variableName:string}}}({{{argsStr:string}}}) {
  const __prompt = {{{promptCode:string}}};
  const startTime = performance.now();
  let __messages = __metadata?.messages || new MessageThread();

  // These are to restore state after interrupt.
  // TODO I think this could be implemented in a cleaner way.
  let __toolCalls = __stateStack.interruptData?.toolCall ? [__stateStack.interruptData.toolCall] : [];
  const __interruptResponse = __stateStack.interruptData?.interruptResponse || null;
  const __tools = {{{tools}}};

  {{#hasResponseFormat}}
  // Need to make sure this is always an object
  const __responseFormat = z.object({
     response: {{{zodSchema:string}}}
  });
  {{/hasResponseFormat}}
  {{^hasResponseFormat}}
  const __responseFormat = undefined;
  {{/hasResponseFormat}}
  
  const __client = __getClientWithConfig({{{clientConfig:string}}});
  let responseMessage;

  if (__toolCalls.length === 0) {
    __messages.push(smoltalk.userMessage(__prompt));
  
  
    await __callHook("onLLMCallStart", { prompt: __prompt, tools: __tools, model: __client.getModel() });
    let __completion = await __client.text({
      messages: __messages.getMessages(),
      tools: __tools,
      responseFormat: __responseFormat,
      stream: {{{isStreaming:boolean}}}
    });

    const endTime = performance.now();

    {{#isStreaming}}
    __completion = await handleStreamingResponse(__completion, statelogClient, __prompt, __toolCalls);
    {{/isStreaming}}

    statelogClient.promptCompletion({
      messages: __messages.getMessages(),
      completion: __completion,
      model: __client.getModel(),
      timeTaken: endTime - startTime,
      tools: __tools,
      responseFormat: __responseFormat
    });

    if (!__completion.success) {
      throw new Error(
        `Error getting response from $\{__model\}: $\{__completion.error\}`
      );
    }

    responseMessage = __completion.value;
    __toolCalls = responseMessage.toolCalls || [];

    if (__toolCalls.length > 0) {
      // Add assistant's response with tool calls to message history
      __messages.push(smoltalk.assistantMessage(responseMessage.output, { toolCalls: __toolCalls }));
    }

    __updateTokenStats(responseMessage.usage, responseMessage.cost);
    await __callHook("onLLMCallEnd", { result: responseMessage, usage: responseMessage.usage, cost: responseMessage.cost, timeTaken: endTime - startTime });

  }

  // Handle function calls
  if (__toolCalls.length > 0) {
    let toolCallStartTime, toolCallEndTime;
    let haltExecution = false;
    let haltToolCall = {}
    let haltInterrupt = null;

    // Process each tool call
    for (const toolCall of __toolCalls) {
      {{{functionCalls:string}}}
    }

    if (haltExecution) {
      statelogClient.debug(`Tool call interrupted execution.`, {
        messages: __messages.getMessages(),
        model: __client.getModel(),
      });

      __stateStack.interruptData = {
        messages: __messages.toJSON().messages,
        nodesTraversed: __graph.getNodesTraversed(),
        toolCall: haltToolCall,
      };
      haltInterrupt.__state = __stateStack.toJSON();
      return haltInterrupt;
    }
  
    const nextStartTime = performance.now();
    await __callHook("onLLMCallStart", { prompt: __prompt, tools: __tools, model: __client.getModel() });
    let __completion = await __client.text({
      messages: __messages.getMessages(),
      tools: __tools,
      responseFormat: __responseFormat,
      stream: {{{isStreaming:boolean}}}
    });

    const nextEndTime = performance.now();

    {{#isStreaming}}
    __completion = await handleStreamingResponse(__completion, statelogClient, __prompt, __toolCalls);
    {{/isStreaming}}

    statelogClient.promptCompletion({
      messages: __messages.getMessages(),
      completion: __completion,
      model: __client.getModel(),
      timeTaken: nextEndTime - nextStartTime,
      tools: __tools,
      responseFormat: __responseFormat,
    });

    if (!__completion.success) {
      throw new Error(
        `Error getting response from $\{__model\}: $\{__completion.error\}`
      );
    }
    responseMessage = __completion.value;
    __updateTokenStats(responseMessage.usage, responseMessage.cost);
    await __callHook("onLLMCallEnd", { result: responseMessage, usage: responseMessage.usage, cost: responseMessage.cost, timeTaken: nextEndTime - nextStartTime });
  }

  // Add final assistant response to history
  // not passing tool calls back this time
  __messages.push(smoltalk.assistantMessage(responseMessage.output));
  {{#hasResponseFormat}}
  try {
  const result = JSON.parse(responseMessage.output || "");
  return result.response;
  } catch (e) {
    return responseMessage.output;
    // console.error("Error parsing response for variable '{{{variableName:string}}}':", e);
    // console.error("Full completion response:", JSON.stringify(__completion, null, 2));
    // throw e;
  }
  {{/hasResponseFormat}}

  {{^hasResponseFormat}}
  return responseMessage.output;
  {{/hasResponseFormat}}
}

{{#isAsync}}
__self.{{{variableName:string}}} = _{{{variableName:string}}}({{{funcCallParams:string}}});
{{/isAsync}}

{{^isAsync}}
__self.{{{variableName:string}}} = await _{{{variableName:string}}}({{{funcCallParams:string}}});

// return early from node if this is an interrupt
if (isInterrupt(__self.{{{variableName:string}}})) {
  {{#nodeContext}}
  return { messages: __threads, data: __self.{{{variableName:string}}} };
  {{/nodeContext}}
   {{^nodeContext}}
   return  __self.{{{variableName:string}}};
   {{/nodeContext}}
}
{{/isAsync}}